import streamlit as st
from llama_index import VectorStoreIndex, ServiceContext
from llama_index.llms import OpenAI
import openai
from llama_index.memory import ChatMemoryBuffer
from llama_index import ServiceContext, VectorStoreIndex
from llama_index.storage.storage_context import StorageContext
from llama_index import KnowledgeGraphIndex, SimpleDirectoryReader
from llama_index import StorageContext
from llama_index.graph_stores.neo4j import Neo4jGraphStore
from llama_index.llms.azure_openai import AzureOpenAI
import os
from llama_index.embeddings.openai import OpenAIEmbedding
import time
# å¯¼å…¥å¿…è¦çš„åº“
from llama_index.llms.openai import OpenAI
from llama_index.evaluation import AnswerRelevancyEvaluator, ContextRelevancyEvaluator



st.set_page_config(page_title="åä¸­å†œä¸šå¤§å­¦é—®ç­”ç³»ç»Ÿ",
                   page_icon="ğŸ¦™",
                   layout="centered",
                   initial_sidebar_state="auto",
                   menu_items=None)

openai.api_key = st.secrets['openai_key']
st.title("ğŸ¦™ åä¸­å†œä¸šå¤§å­¦é—®ç­”ç³»ç»Ÿ")
st.info("æŸ¥è¯¢åŸºäºçŸ¥è¯†å›¾è°±ï¼Œç”±å­¦æ ¡å®˜ç½‘æ•´ç†åˆ¶ä½œè€Œæˆ", icon="ğŸ“ƒ")

if "messages" not in st.session_state.keys(): # Initialize the chat messages history
    st.session_state.messages = [
        {"role": "assistant", "content": "Ask me a question about HZAU!"}
    ]

@st.cache_resource(show_spinner=False)
def load_data_kg():
    with st.spinner(text="Loading and indexing the docs. This should take 1-2 minutes."):
        # load documents
        #llm = OpenAI(model="gpt-3.5-turbo", temperature=0.5, system_prompt="You are an expert on the LlamaIndex docs and your job is to answer user's questions. Assume that all questions are related to the LlamaIndex docs. Keep your answers being based on facts â€“ do not hallucinate features.Your answer is output in Chinese")

        llm = AzureOpenAI(
            model="gpt-35-turbo-16k",
            deployment_name="gpt-35-turbo-16k",
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_version="2024-02-15-preview",
            system_prompt="æ‰€æœ‰ä½ è¾“å‡ºçš„æ•°æ®éƒ½å¿…é¡»æ˜¯ä¸­æ–‡ï¼Œå¤„ç†æ•°æ®æ—¶ä¹Ÿç”¨ä¸­æ–‡è¡¨ç¤º"
        )

        embedding_llm = OpenAIEmbedding(
            model="text-embedding-ada-002",
            deployment_name="text-embedding-ada-002",
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_version="2024-02-15-preview",
        )

        username = "neo4j"
        password = "689100"
        url = "bolt://localhost:7687"
        database = "Neo4j"

        graph_store = Neo4jGraphStore(
            username=username,
            password=password,
            url=url,
            database=database,
        )
        storage_context = StorageContext.from_defaults(graph_store=graph_store)

        from llama_index.query_engine import KnowledgeGraphQueryEngine
        from llama_index.query_engine.knowledge_graph_query_engine import DEFAULT_NEO4J_NL2CYPHER_PROMPT
        query_engine = KnowledgeGraphQueryEngine(
            storage_context=storage_context,
            llm=llm,
            embedding_llm=embedding_llm,
            verbose=True,
            graph_query_synthesis_prompt=DEFAULT_NEO4J_NL2CYPHER_PROMPT
        )

    return query_engine


@st.cache_resource(show_spinner=False)
def load_data():
    with st.spinner(text="Loading and indexing the docs. This should take 1-2 minutes."):
        # load documents
        #llm = OpenAI(model="gpt-3.5-turbo", temperature=0.5, system_prompt="You are an expert on the LlamaIndex docs and your job is to answer user's questions. Assume that all questions are related to the LlamaIndex docs. Keep your answers being based on facts â€“ do not hallucinate features.Your answer is output in Chinese")
        llm = OpenAI(model="gpt-3.5-turbo", system_prompt="æ‰€æœ‰è¾“å‡ºçš„æ•°æ®éƒ½å¿…é¡»æ˜¯ä¸­æ–‡ï¼Œå¤„ç†æ•°æ®æ—¶ä¹Ÿç”¨ä¸­æ–‡è¡¨ç¤ºï¼Œä¸€åˆ‡å‚æ•°ä¹Ÿä½¿ç”¨ä¸­æ–‡è¿›è¡Œè¡¨ç¤º;Keep your answers being based on facts â€“ do not hallucinate features.Extrapolate as many relationships as you can from the prompt and generate tuples like (source, relation, target). Make sure there are always source, relation and target in the tuple.Example:prompt: John knows React, Golang, and Python. John is good at Software Engineering and Leadership.tuple: (John, knows, React); (John, knows, Golang); (John, knows, Python); (John, good_at, Software_Engineering); (John, good_at, Leadership)")
        embedding_llm = OpenAIEmbedding(
            model="text-embedding-3-large",
            deployment_name="text-embedding-3-large",
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_version="2024-02-15-preview",
        )

        #embed_model="local:BAAI/bge-small-en-v1.5"

        service_context = ServiceContext.from_defaults(
            llm=llm,
            #embed_model=embed_model,
            embed_model=embedding_llm
        )
        documents = SimpleDirectoryReader(input_files=["./hzau.txt"]).load_data()

        username = "neo4j"
        password = "689100"
        url = "bolt://localhost:7687"
        database = "Neo4j"

        graph_store = Neo4jGraphStore(
            username=username,
            password=password,
            url=url,
            database=database,
        )
        storage_context = StorageContext.from_defaults(graph_store=graph_store)


        QueryEngine = KnowledgeGraphIndex.from_documents(
            documents,
            service_context=service_context,
            storage_context=storage_context,
            max_triplets_per_chunk=10,
        )

        return QueryEngine

index = load_data()
query_engine_kg = load_data_kg()

stream = st.checkbox("ä»…é‡‡ç”¨çŸ¥è¯†å›¾è°±è¿›è¡ŒæŸ¥è¯¢", value=False)  # Add a checkbox for streaming

if "chat_engine" not in st.session_state.keys(): # Initialize the chat engine
        #st.session_state.chat_engine = index.as_chat_engine(chat_mode="condense_plus_context", verbose=True,memory=ChatMemoryBuffer.from_defaults(token_limit=10000))
        st.session_state.chat_engine = index.as_chat_engine(chat_mode="openai", verbose=True, system_prompt="æ‰€æœ‰è¾“å‡ºçš„æ•°æ®éƒ½å¿…é¡»æ˜¯ä¸­æ–‡ï¼Œå¤„ç†æ•°æ®æ—¶ä¹Ÿç”¨ä¸­æ–‡è¡¨ç¤º")

if prompt := st.chat_input("Enter a prompt here"): # Prompt for user input and save to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages: # Display the prior chat messages
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    """Clears the chat history and resets the state."""
    st.session_state.messages = [
         {"role": "assistant", "content": "Ask me a question about HZAU documentation!"}
         ]
    del st.session_state["chat_engine"]
# Create the sidebar
st.sidebar.header("Options")

# Add the "Clear History" button to the sidebar
if st.sidebar.button("Clear History", key="clear"):
    clear_chat_history()


# åˆå§‹åŒ–è¯„ä¼°å™¨
answer_evaluator = AnswerRelevancyEvaluator(service_context = ServiceContext.from_defaults(
            llm=OpenAI(model="gpt-3.5-turbo"),
        ))
context_evaluator = ContextRelevancyEvaluator(service_context = ServiceContext.from_defaults(
            llm=OpenAI(model="gpt-3.5-turbo"),
        ))


# If last message is not from assistant, generate a new response
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            if stream:
                response = query_engine_kg.query(prompt)
                st.write(response.response)
                message = {"role": "assistant", "content": response.response}
                st.session_state.messages.append(message)  # Add response to message history
            else:
                #
                start_time=time.time()
                response = st.session_state.chat_engine.chat(prompt)
                st.write(response.response)
                end_time=time.time()
                answer_time=end_time-start_time
                print("æ‰€ç”¨æ—¶é—´ä¸º: "+str(answer_time))
                #
                message = {"role": "assistant", "content": response.response}
                st.session_state.messages.append(message) # Add response to message history

                # åœ¨ä½ çš„é—®ç­”é€»è¾‘åæ·»åŠ è¯„ä¼°ä»£ç 
                answer_score = answer_evaluator.evaluate(prompt, str(response.response))
                # context_score = context_evaluator.evaluate(query, reference_context)

                # è¾“å‡ºè¯„ä¼°å¾—åˆ†
                print(f"Answer Relevancy Score: {answer_score.score}")
                # print(f"Context Relevancy Score: {context_score}")
